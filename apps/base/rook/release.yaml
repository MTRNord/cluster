---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: rook-ceph
  namespace: rook-ceph
spec:
  interval: 5m
  chart:
    spec:
      version: "1.11.x"
      chart: rook-ceph
      sourceRef:
        kind: HelmRepository
        name: rook-ceph
      interval: 60m
  install:
    crds: Create
  upgrade:
    crds: CreateReplace
  values:
    crds:
      enabled: false
    monitoring:
      enabled: true
---
apiVersion: helm.toolkit.fluxcd.io/v2beta1
kind: HelmRelease
metadata:
  name: rook-ceph-cluster
  namespace: rook-ceph
spec:
  interval: 5m
  chart:
    spec:
      version: "1.11.x"
      chart: rook-ceph-cluster
      sourceRef:
        kind: HelmRepository
        name: rook-ceph
      interval: 60m
  install:
    crds: Create
  upgrade:
    crds: CreateReplace
  values:
    toolbox:
      enabled: false
    monitoring:
      enabled: true
      createPrometheusRules: true
    cephClusterSpec:
      cephVersion:
        image: quay.io/ceph/ceph:v17.2.6
      mon:
        # Set the number of mons to be started. Generally recommended to be 3.
        # For highest availability, an odd number of mons should be specified.
        count: 3
        # The mons should be on unique nodes. For production, at least 3 nodes are recommended for this reason.
        # Mons should only be allowed on the same node for test environments where data loss is acceptable.
        allowMultiplePerNode: false
      mgr:
        # When higher availability of the mgr is needed, increase the count to 2.
        # In that case, one mgr will be active and one in standby. When Ceph updates which
        # mgr is active, Rook will update the mgr services to match the active mgr.
        count: 2
        allowMultiplePerNode: false
        modules:
          # Several modules should not need to be included in this list. The "dashboard" and "monitoring" modules
          # are already enabled by other settings in the cluster CR.
          - name: pg_autoscaler
            enabled: true

      # enable the ceph dashboard for viewing cluster status
      dashboard:
        enabled: true
        # serve the dashboard under a subpath (useful when you are accessing the dashboard via a reverse proxy)
        # urlPrefix: /ceph-dashboard
        # serve the dashboard at the given port.
        # port: 8443
        # Serve the dashboard using SSL (if using ingress to expose the dashboard and `ssl: true` you need to set
        # the corresponding "backend protocol" annotation(s) for your ingress controller of choice)
        ssl: false
      storage:
        useAllNodes: false
        nodes:
          - name: "control-plane-2"
            devices:
              - name: "sdb"
          - name: "control-plane-3"
            devices:
              - name: "sdb"
    placement:
      all:
        nodeAffinity: null
        tolerations:
          - effect: NoSchedule
            key: node-role.kubernetes.io/control-plane
            operator: Exists
    resources:
      mgr:
        limits:
          cpu: "1000m"
          memory: "1Gi"
        requests:
          cpu: "0m"
          memory: "512Mi"
      mon:
        limits:
          cpu: "2000m"
          memory: "2Gi"
        requests:
          cpu: "0m"
          memory: "1Gi"
      osd:
        limits:
          cpu: "2000m"
          memory: "4Gi"
        requests:
          cpu: "0m"
          memory: "4Gi"
      prepareosd:
        # limits: It is not recommended to set limits on the OSD prepare job
        #         since it's a one-time burst for memory that must be allowed to
        #         complete without an OOM kill.  Note however that if a k8s
        #         limitRange guardrail is defined external to Rook, the lack of
        #         a limit here may result in a sync failure, in which case a
        #         limit should be added.  1200Mi may suffice for up to 15Ti
        #         OSDs ; for larger devices 2Gi may be required.
        #         cf. https://github.com/rook/rook/pull/11103
        requests:
          cpu: "0m"
          memory: "50Mi"
      mgr-sidecar:
        limits:
          cpu: "500m"
          memory: "100Mi"
        requests:
          cpu: "0m"
          memory: "40Mi"
      crashcollector:
        limits:
          cpu: "500m"
          memory: "60Mi"
        requests:
          cpu: "0m"
          memory: "60Mi"
      logcollector:
        limits:
          cpu: "500m"
          memory: "1Gi"
        requests:
          cpu: "0m"
          memory: "100Mi"
      cleanup:
        limits:
          cpu: "500m"
          memory: "1Gi"
        requests:
          cpu: "0m"
          memory: "100Mi"
    ingress:
      # -- Enable an ingress for the ceph-dashboard
      dashboard:
        annotations:
          traefik.ingress.kubernetes.io/router.middlewares: traefik-ingress-sso@kubernetescrd
          cert-manager.io/cluster-issuer: letsencrypt-dns
        host:
          name: dashboard.ceph.midnightthoughts.space
          #path: "/ceph-dashboard(/|$)(.*)"
          path: /
        tls:
        - hosts:
            - dashboard.ceph.midnightthoughts.space
          secretName: dashboard.ceph.midnightthoughts.space-tls
    cephBlockPools: []
      # - name: ceph-blockpool
      #   # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Block-Storage/ceph-block-pool-crd.md#spec for available configuration
      #   spec:
      #     failureDomain: host
      #     replicated:
      #       size: 2
      #     # Enables collecting RBD per-image IO statistics by enabling dynamic OSD performance counters. Defaults to false.
      #     # For reference: https://docs.ceph.com/docs/master/mgr/prometheus/#rbd-io-statistics
      #     # enableRBDStats: true
      #   storageClass:
      #     enabled: true
      #     name: ceph-block
      #     isDefault: false
      #     reclaimPolicy: Retain
      #     allowVolumeExpansion: true
      #     volumeBindingMode: "Immediate"
      #     mountOptions: []
      #     # see https://kubernetes.io/docs/concepts/storage/storage-classes/#allowed-topologies
      #     allowedTopologies: []
      #     #        - matchLabelExpressions:
      #     #            - key: rook-ceph-role
      #     #              values:
      #     #                - storage-node
      #     # see https://github.com/rook/rook/blob/master/Documentation/ceph-block.md#provision-storage for available configuration
      #     parameters:
      #       # (optional) mapOptions is a comma-separated list of map options.
      #       # For krbd options refer
      #       # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options
      #       # For nbd options refer
      #       # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options
      #       # mapOptions: lock_on_read,queue_depth=1024

      #       # (optional) unmapOptions is a comma-separated list of unmap options.
      #       # For krbd options refer
      #       # https://docs.ceph.com/docs/master/man/8/rbd/#kernel-rbd-krbd-options
      #       # For nbd options refer
      #       # https://docs.ceph.com/docs/master/man/8/rbd-nbd/#options
      #       # unmapOptions: force

      #       # RBD image format. Defaults to "2".
      #       imageFormat: "2"

      #       # RBD image features, equivalent to OR'd bitfield value: 63
      #       # Available for imageFormat: "2". Older releases of CSI RBD
      #       # support only the `layering` feature. The Linux kernel (KRBD) supports the
      #       # full feature complement as of 5.4
      #       imageFeatures: layering

      #       # These secrets contain Ceph admin credentials.
      #       csi.storage.k8s.io/provisioner-secret-name: rook-csi-rbd-provisioner
      #       csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
      #       csi.storage.k8s.io/controller-expand-secret-name: rook-csi-rbd-provisioner
      #       csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
      #       csi.storage.k8s.io/node-stage-secret-name: rook-csi-rbd-node
      #       csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
      #       # Specify the filesystem type of the volume. If not specified, csi-provisioner
      #       # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
      #       # in hyperconverged settings where the volume is mounted on the same node as the osds.
      #       csi.storage.k8s.io/fstype: ext4
    # -- A list of CephFileSystem configurations to deploy
    # @default -- See [below](#ceph-file-systems)
    cephFileSystems:
      - name: ceph-filesystem
        # see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#filesystem-settings for available configuration
        spec:
          metadataPool:
            replicated:
              size: 2
          dataPools:
            - failureDomain: host
              replicated:
                size: 2
              # Optional and highly recommended, 'data0' by default, see https://github.com/rook/rook/blob/master/Documentation/CRDs/Shared-Filesystem/ceph-filesystem-crd.md#pools
              name: data0
          metadataServer:
            activeCount: 1
            activeStandby: true
            resources:
              limits:
                cpu: "2000m"
                memory: "4Gi"
              requests:
                cpu: "0m"
                memory: "4Gi"
            priorityClassName: system-cluster-critical
        storageClass:
          enabled: true
          isDefault: false
          name: ceph-filesystem
          # (Optional) specify a data pool to use, must be the name of one of the data pools above, 'data0' by default
          pool: data0
          reclaimPolicy: Retain
          allowVolumeExpansion: true
          volumeBindingMode: "Immediate"
          mountOptions: []
          # see https://github.com/rook/rook/blob/master/Documentation/ceph-filesystem.md#provision-storage for available configuration
          parameters:
            # The secrets contain Ceph admin credentials.
            csi.storage.k8s.io/provisioner-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/provisioner-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/controller-expand-secret-name: rook-csi-cephfs-provisioner
            csi.storage.k8s.io/controller-expand-secret-namespace: "{{ .Release.Namespace }}"
            csi.storage.k8s.io/node-stage-secret-name: rook-csi-cephfs-node
            csi.storage.k8s.io/node-stage-secret-namespace: "{{ .Release.Namespace }}"
            # Specify the filesystem type of the volume. If not specified, csi-provisioner
            # will set default as `ext4`. Note that `xfs` is not recommended due to potential deadlock
            # in hyperconverged settings where the volume is mounted on the same node as the osds.
            csi.storage.k8s.io/fstype: ext4
    cephObjectStores: []
