apiVersion: v1
kind: Namespace
metadata:
    name: rook-ceph
---
apiVersion: source.toolkit.fluxcd.io/v1beta2
kind: HelmRepository
metadata:
    name: rook-ceph
    namespace: rook-ceph
spec:
    interval: 24h
    url: https://charts.rook.io/release
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
    name: rook-ceph-operator
    namespace: rook-ceph
spec:
    chart:
        spec:
            chart: rook-ceph
            sourceRef:
                kind: HelmRepository
                name: rook-ceph
    interval: 60m
    values:
        csi:
            enableCephfsSnapshotter: false
            enableNFSSnapshotter: false
            enableRBDSnapshotter: false
            enableVolumeGroupSnapshot: false
            serviceMonitor:
                enabled: true
        monitoring:
            enabled: true
---
apiVersion: helm.toolkit.fluxcd.io/v2
kind: HelmRelease
metadata:
    name: rook-ceph-cluster
    namespace: rook-ceph
spec:
    chart:
        spec:
            chart: rook-ceph-cluster
            sourceRef:
                kind: HelmRepository
                name: rook-ceph
            version: v1.17.2
    interval: 60m
    values:
        toolbox:
            enabled: true
        monitoring:
            enabled: true
            createPrometheusRules: true
        cephClusterSpec:
            cephVersion:
                image: quay.io/ceph/ceph:v19.2.2
            mgr:
                count: 2
                modules:
                    - name: rook
                      enabled: true
            mon:
                count: 2
                allowMultiplePerNode: false
                volumeClaimTemplate:
                    spec:
                        storageClassName: hcloud-volumes
                        resources:
                            requests:
                                storage: 10Gi
                        accessModes:
                            - ReadWriteOnce
            storage:
                useAllNodes: true
                #useAllDevices: false
                # Helps rebalance of OSD CRUSH after pvc size increase
                allowOsdCrushWeightUpdate: true
                storageClassDeviceSets:
                    - name: "hcloud-volumes"
                      count: 3
                      portable: true
                      encrypted: false
                      tuneFastDeviceClass: true
                      # Since the OSDs could end up on any node, an effort needs to be made to spread the OSDs
                      # across nodes as much as possible. Unfortunately the pod anti-affinity breaks down
                      # as soon as you have more than one OSD per node. The topology spread constraints will
                      # give us an even spread on K8s 1.18 or newer.
                      placement:
                        nodeAffinity:
                            requiredDuringSchedulingIgnoredDuringExecution:
                                nodeSelectorTerms:
                                    - matchExpressions:
                                        - key: midnightthoughts.rook.ceph/allowed
                                          operator: In
                                          values:
                                            - "true"
                        topologySpreadConstraints:
                            - maxSkew: 1
                              topologyKey: kubernetes.io/hostname
                              whenUnsatisfiable: ScheduleAnyway
                              labelSelector:
                                matchExpressions:
                                    - key: app
                                      operator: In
                                      values:
                                        - rook-ceph-osd
                        preparePlacement:
                            nodeAffinity:
                                requiredDuringSchedulingIgnoredDuringExecution:
                                    nodeSelectorTerms:
                                        - matchExpressions:
                                            - key: midnightthoughts.rook.ceph/allowed
                                              operator: In
                                              values:
                                                - "true"
                            podAntiAffinity:
                                preferredDuringSchedulingIgnoredDuringExecution:
                                    - weight: 100
                                      podAffinityTerm:
                                      topologyKey: kubernetes.io/hostname
                                      labelSelector:
                                        matchExpressions:
                                            - key: app
                                              operator: In
                                              values:
                                                - rook-ceph-osd
                                            - key: app
                                              operator: In
                                              values:
                                                - rook-ceph-osd-prepare
                            topologySpreadConstraints:
                                - maxSkew: 1
                                  topologyKey: topology.kubernetes.io/zone
                                  whenUnsatisfiable: DoNotSchedule
                                  labelSelector:
                                    matchExpressions:
                                        - key: app
                                          operator: In
                                          values:
                                            - rook-ceph-osd-prepare
                      volumeClaimTemplates:
                        - metadata:
                            name: "data"
                          spec:
                            storageClassName: hcloud-volumes
                            resources:
                                requests:
                                    storage: 120Gi
                            volumeMode: Block
                            accessModes:
                                - ReadWriteOnce
            dashboard:
                enabled: true
                ssl: false
        ingress:
            dashboard:
                host: false
        cephBlockPools: []
        cephObjectStores: []
        cephFilesystems:
            - name: cephfs
              spec:
                metadataPool:
                    replicated:
                        size: 2
                        requireSafeReplicaSize: false
                    parameters:
                        compression_mode: force
                dataPools:
                    - name: data0
                      replicated:
                        size: 2
                        requireSafeReplicaSize: false
                      parameters:
                        compression_mode: force
                preserveFilesystemOnDelete: true
              storageClass:
                enabled: true
                isDefault: true
                name: cephfs
                pool: data0
                reclaimPolicy: Delete
                allowVolumeExpansion: true
                volumeBindingMode: Immediate
---
apiVersion: gateway.networking.k8s.io/v1
kind: HTTPRoute
metadata:
    name: rook-ceph-dashboard
    namespace: rook-ceph
spec:
    parentRefs:
        - name: envoy-gateway
          namespace: envoy-gateway
    hostnames:
        - rook.midnightthoughts.space
    rules:
        - backendRefs:
            - name: rook-ceph-mgr-dashboard
              port: 7000
          timeouts:
            request: 240s
            backendRequest: 0s
